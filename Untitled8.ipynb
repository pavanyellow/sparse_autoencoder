{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5alXWLDoz7C6",
        "outputId": "26fb8574-5f15-4755-fd45-4062b659b647"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/sparse_autoencoder.git\n",
            "  Cloning https://github.com/openai/sparse_autoencoder.git to /tmp/pip-req-build-xt5w7rpc\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/sparse_autoencoder.git /tmp/pip-req-build-xt5w7rpc\n",
            "  Resolved https://github.com/openai/sparse_autoencoder.git to commit 8f74a1cbeb15a6a7e082c812ccc5055045256bb4\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting blobfile==2.0.2 (from sparse_autoencoder==0.1)\n",
            "  Downloading blobfile-2.0.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch==2.1.0 in /usr/local/lib/python3.10/dist-packages (from sparse_autoencoder==0.1) (2.1.0+cu121)\n",
            "Collecting transformer-lens==1.9.1 (from sparse_autoencoder==0.1)\n",
            "  Downloading transformer_lens-1.9.1-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycryptodomex~=3.8 (from blobfile==2.0.2->sparse_autoencoder==0.1)\n",
            "  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from blobfile==2.0.2->sparse_autoencoder==0.1) (2.0.7)\n",
            "Requirement already satisfied: lxml~=4.9 in /usr/local/lib/python3.10/dist-packages (from blobfile==2.0.2->sparse_autoencoder==0.1) (4.9.4)\n",
            "Requirement already satisfied: filelock~=3.0 in /usr/local/lib/python3.10/dist-packages (from blobfile==2.0.2->sparse_autoencoder==0.1) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->sparse_autoencoder==0.1) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->sparse_autoencoder==0.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->sparse_autoencoder==0.1) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->sparse_autoencoder==0.1) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->sparse_autoencoder==0.1) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->sparse_autoencoder==0.1) (2.1.0)\n",
            "Collecting accelerate>=0.23.0 (from transformer-lens==1.9.1->sparse_autoencoder==0.1)\n",
            "  Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting beartype<0.15.0,>=0.14.1 (from transformer-lens==1.9.1->sparse_autoencoder==0.1)\n",
            "  Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.7.1 (from transformer-lens==1.9.1->sparse_autoencoder==0.1)\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops>=0.6.0 (from transformer-lens==1.9.1->sparse_autoencoder==0.1)\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fancy-einsum>=0.0.3 (from transformer-lens==1.9.1->sparse_autoencoder==0.1)\n",
            "  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
            "Collecting jaxtyping>=0.2.11 (from transformer-lens==1.9.1->sparse_autoencoder==0.1)\n",
            "  Downloading jaxtyping-0.2.25-py3-none-any.whl (39 kB)\n",
            "Collecting numpy>=1.24 (from transformer-lens==1.9.1->sparse_autoencoder==0.1)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12>=12.1.3.1 (from transformer-lens==1.9.1->sparse_autoencoder==0.1)\n",
            "  Downloading nvidia_cublas_cu12-12.3.4.1-py3-none-manylinux1_x86_64.whl (412.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.6/412.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12>=12.1.105 (from transformer-lens==1.9.1->sparse_autoencoder==0.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (14.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12>=12.1.105 (from transformer-lens==1.9.1->sparse_autoencoder==0.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl (24.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12>=12.1.105 (from transformer-lens==1.9.1->sparse_autoencoder==0.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (867 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.7/867.7 kB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12>=8.9.2.26 (from transformer-lens==1.9.1->sparse_autoencoder==0.1)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.7.29-py3-none-manylinux1_x86_64.whl (704.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m704.7/704.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12>=11.0.2.54 (from transformer-lens==1.9.1->sparse_autoencoder==0.1)\n",
            "  Downloading nvidia_cufft_cu12-11.0.12.1-py3-none-manylinux1_x86_64.whl (98.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.8/98.8 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12>=10.3.2.106 (from transformer-lens==1.9.1->sparse_autoencoder==0.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.4.107-py3-none-manylinux1_x86_64.whl (56.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12>=11.4.5.107 (from transformer-lens==1.9.1->sparse_autoencoder==0.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.5.4.101-py3-none-manylinux1_x86_64.whl (125.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12>=12.1.0.106 (from transformer-lens==1.9.1->sparse_autoencoder==0.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.2.0.103-py3-none-manylinux1_x86_64.whl (197.5 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m164.9/197.5 MB\u001b[0m \u001b[31m117.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install git+https://github.com/openai/sparse_autoencoder.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformer_lens\n",
        "from transformer_lens import utils\n",
        "import torch\n",
        "import tqdm\n",
        "import torch\n",
        "import blobfile as bf\n",
        "import transformer_lens\n",
        "import sparse_autoencoder\n",
        "# Load a model (eg GPT-2 Small)\n",
        "model = transformer_lens.HookedTransformer.from_pretrained(\"gpt2-small\")"
      ],
      "metadata": {
        "id": "WsfEAt-7z_tS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b04b9c60-f6a5-470d-fd92-3f3891817e7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model gpt2-small into HookedTransformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import einops\n",
        "\n",
        "\n",
        "from torch import nn\n",
        "import os, sys\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "from transformer_lens import HookedTransformer, utils\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from typing import Dict\n",
        "from tqdm.notebook import tqdm\n",
        "import plotly.express as px\n",
        "import json\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "def get_batch_tokens(dataset_iter, batch_size, model, ctx_len):\n",
        "    tokens = []\n",
        "    total_tokens = 0\n",
        "    while total_tokens < batch_size*ctx_len:\n",
        "        try:\n",
        "            # Retrieve next item from iterator\n",
        "            row = next(dataset_iter)[\"text\"]\n",
        "        except StopIteration:\n",
        "            # Break the loop if dataset ends\n",
        "            break\n",
        "\n",
        "        # Tokenize the text with a check for max_length\n",
        "        cur_toks = model.to_tokens(row)\n",
        "        tokens.append(cur_toks)\n",
        "\n",
        "        total_tokens += cur_toks.numel()\n",
        "\n",
        "    # Check if any tokens were collected\n",
        "    if not tokens:\n",
        "        return None\n",
        "\n",
        "    # Depending on your model's tokenization, you might need to pad the tokens here\n",
        "\n",
        "    # Flatten the list of tokens\n",
        "    flat_tokens = torch.cat(tokens, dim=-1).flatten()\n",
        "    flat_tokens = flat_tokens[:batch_size * ctx_len]\n",
        "    reshaped_tokens = einops.rearrange(\n",
        "        flat_tokens,\n",
        "        \"(batch seq_len) -> batch seq_len\",\n",
        "        batch=batch_size,\n",
        "    )\n",
        "    reshaped_tokens[:, 0] = model.tokenizer.bos_token_id\n",
        "    return reshaped_tokens\n",
        "\n",
        "def shuffle_data(all_tokens):\n",
        "    print(\"Shuffled data\")\n",
        "    return all_tokens[torch.randperm(all_tokens.shape[0])]\n",
        "\n",
        "dataset = load_dataset('Skylion007/openwebtext', split='train', streaming=True)\n",
        "dataset_iter = iter(dataset)\n",
        "ctx_len = 128\n",
        "all_tokens_batches = int(1e6) // ctx_len\n",
        "\n",
        "all_tokens = get_batch_tokens(dataset_iter, all_tokens_batches, model, ctx_len)\n",
        "all_tokens = shuffle_data(all_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5pVZ6fw2Hxx",
        "outputId": "d21ea239-4e7a-4a33-f28e-0dc5d0a40769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1429: FutureWarning: The repository for Skylion007/openwebtext contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/Skylion007/openwebtext\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shuffled data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt_oUoq-2rzt",
        "outputId": "d34f17e4-dc53-4d2b-fda2-fcea043eb1b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([7812, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the autoencoder\n",
        "layer_index = 0  # in range(12)\n",
        "autoencoder_input = [\"mlp_post_act\", \"resid_delta_mlp\"][1]\n",
        "filename = f\"az://openaipublic/sparse-autoencoder/gpt2-small/{autoencoder_input}/autoencoders/{layer_index}.pt\"\n",
        "with bf.BlobFile(filename, mode=\"rb\") as f:\n",
        "    state_dict = torch.load(f)\n",
        "    autoencoder = sparse_autoencoder.Autoencoder.from_state_dict(state_dict)\n",
        "model = transformer_lens.HookedTransformer.from_pretrained(\"gpt2\", center_writing_weights=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HRdlIM-2upp",
        "outputId": "bc390ab6-bfeb-47f4-80e2-06abd4ec8fb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model gpt2 into HookedTransformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "layer_idx = 0\n",
        "acts_key = f'blocks.{layer_idx}.hook_mlp_out'\n",
        "batches = 16\n",
        "context_length = 128\n",
        "\n",
        "cpu = torch.device('cpu')\n",
        "cuda = torch.device('cuda')\n",
        "from tqdm import tqdm\n",
        "resid_acts = torch.tensor([], device = cpu)\n",
        "latents_acts = torch.tensor([], device = cpu)\n",
        "autoencoder.to(cuda)\n",
        "with torch.no_grad():\n",
        "    for i in tqdm(range(batches)):\n",
        "        logits, cache = model.run_with_cache(all_tokens[i*batch_size:(i+1)*batch_size],stop_at_layer = layer_idx +1)\n",
        "        resid_cache = cache[acts_key][:,:context_length-1,:]\n",
        "        resid_acts = torch.cat((resid_acts, resid_cache.to(cpu)))\n",
        "\n",
        "\n",
        "        latent_activations = autoencoder.encode(resid_cache)\n",
        "        latents_acts = torch.cat((latents_acts, latent_activations.to(cpu)))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c8iuLZc3AHg",
        "outputId": "7f9af881-d31b-4923-e0c4-5143a8a3c6ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:43<00:00,  2.71s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(resid_acts.shape, latents_acts.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyvpKoYj3K6m",
        "outputId": "56248ded-3e6b-4d76-836f-1ca4fc8be4da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1024, 127, 768]) torch.Size([1024, 127, 32768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = torch.tensor([], device= cpu)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in  tqdm(range(batches)):\n",
        "        curr_loss = model(all_tokens[i*batch_size:(i+1)*batch_size], return_type = 'loss', loss_per_token = True).view(-1)\n",
        "        loss = torch.cat((loss, curr_loss.to(cpu)))\n",
        "\n",
        "print(loss.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCTe_-CB49RV",
        "outputId": "659e93a9-8978-4840-cb38-5a4d57c496e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:11<00:00,  1.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([130048])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2CMCrZF5IBM",
        "outputId": "f9accac4-0ee6-4935-aca2-c2e13dd31ab4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([130048])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZr2Q2i35ujr",
        "outputId": "fcb76f0e-d63b-493d-f8e6-43f17868591c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.5796)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latents_acts = latents_acts[:,:,:500].to(cuda)"
      ],
      "metadata": {
        "id": "AmqtO0xY6HOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a,b, c = latents_acts.shape\n",
        "latents_acts = latents_acts.reshape(a*b, c)"
      ],
      "metadata": {
        "id": "aajTIyAA8htm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latents_acts.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1_wFfyg80-q",
        "outputId": "9df5f31b-ec72-45bd-b419-446157246afe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([130048, 32768])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(latents_acts > 0).sum(dim = 0)"
      ],
      "metadata": {
        "id": "uT8radbs83My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latents_acts.numel()*4/1e6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdX62v3O9CIT",
        "outputId": "7ae0de24-b1d4-47bc-84b9-67b7a184821a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "260.096"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(loss.numel()*4)/1e9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzlpCuSD-2Xm",
        "outputId": "0ff35388-4edc-4673-9710-623f94c883e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.000520192"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(all_tokens.numel()*4)/1e6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxX7AHOJ_FyZ",
        "outputId": "e83633e4-b87c-4d39-b36c-8c6bd7d09745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.999744"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latents_acts.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBf2LDFk_L_-",
        "outputId": "5f6bb9b6-91ea-47b9-dbef-31e02c311629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1024, 127, 500])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.shape"
      ],
      "metadata": {
        "id": "oINzRsBS_unu",
        "outputId": "b8aec03d-a9c9-490f-9fea-18aacabf2665",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([130048])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G7R4My1Q_yUO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}